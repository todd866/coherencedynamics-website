---
slug: coherence-dynamics-circa-1890
title: Coherence Dynamics Circa 1890
subtitle: Why Bergson and James were right about the "Soup," and how the 20th century got lost in the "Sparks"
date: 2025-12-19
tags:
  - philosophy of mind
  - history
  - consciousness
  - dimensionality
summary: The mathematical tools to formalize Bergson's "holographic brain" and James's "transmission" hypothesis have existed for decades. We just forgot to use them.
relatedPapers:
  - substrate-dimensionality
  - high-dimensional-coherence
  - coherence-time
---

Ned Block's recent paper ["Can only meat machines be conscious?"](https://doi.org/10.1016/j.tics.2025.08.009) (*Trends in Cognitive Sciences*, 2025) poses a difficult dilemma for functionalism: if consciousness is just computation (roles), why does it seem so tied to biological "meat" (realizers)? He highlights a specific evolutionary puzzle: purely electrical nervous systems ("sparks") are fast and reliable, yet evolution heavily favored electrochemical systems ("soup") for complex cognition.

Block asks: *What is special about electrochemical computation?*

He considers inhibition, learning, and various biological factors, but finds them inconclusive. As he notes, the "War of the Soups and the Sparks" (Valenstein's phrase) was supposedly settled in 1921 when Otto Loewi showed chemical transmission existed---but we still don't know *why* evolution favored the messy, slow "soup" over the fast, reliable "sparks."

I want to propose a simpler answer: **dimensionality**.

> **What I mean by dimensionality:** Not the number of neurons, nor the size of the state space (a digital computer's microstate count is astronomically large). I mean *effective dimensionality*: the number of independent degrees of freedom the system can reliably occupy and be steered through on timescales relevant to behavior. A transistor array has billions of *microstates*, but its macroscopic behavior is constrained to rigid, serial logic gates. An electrochemical system has fewer components but a vastly richer manifold of reachable dynamical regimes. The difference isn't size---it's *navigability*.

---

## The Wrong Metric: Speed vs. Dimensionality

If we view the brain as a digital computer, the evolutionary choice makes no sense. Electrical gap junctions are faster than chemical synapses. But the brain isn't optimizing for speed; it's optimizing for dimensionality.

**The Transistor (Digital/AI):** To simulate a high-dimensional vector space (like an LLM embedding) on silicon, we must force billions of low-dimensional switches (transistors) to perform matrix math. This is energetically expensive and computationally brittle. We are "faking" the manifold. Crucially, a simulation of an attractor is a discrete, step-by-step calculation; a physical attractor is a *continuous relaxation* into an energy minimum. Continuity isn't just a mathematical abstraction---it's a physical property that digital systems must approximate rather than instantiate.

**The Oscillator (Analog/Biology):** An electrochemical system involves coupled oscillators (neurons in a chemical bath). Physics dictates that these oscillators settle into phase-locked states or complex attractors *naturally*. The high-dimensional dynamics are *native* to the substrate---the system explores its state space as a consequence of relaxing into its energy landscape, without needing to emulate it step-by-step.

The central thesis: **Oscillators render high-dimensional dynamics more efficiently than transistors.**

<!-- SIMULATION: soup-vs-sparks -->

A chemical synapse isn't just a switch; it's a high-dimensional control surface. Consider the difference: an electrical synapse (gap junction) is characterized by a handful of parameters---coupling conductance, maybe some rectification. A chemical synapse exposes a *vector* of independently tunable knobs: transmitter identity and mixture, release probability, receptor subtype distribution, metabotropic cascades, local uptake and clearance, short-term plasticity, long-term plasticity, neuromodulator context, volume transmission, multiple time constants. Chemical synapses buy *controllability* and *context-dependent reconfiguration*---a larger reachable set of dynamical regimes---at the cost of speed. By modulating the "soup," nature created a substrate that allows low-dimensional signals (spikes) to *steer* a massive, continuous high-dimensional manifold.

---

## The "Transmission" Hypothesis (William James, 1898)

Start with William James. He famously argued that the brain does not *produce* consciousness (like a generator produces steam); it *transmits* it (like a prism transmits light or a lens focuses it).

**The 1890 View:** The brain is a "reducing valve" (to use Aldous Huxley's later phrase) that filters a larger consciousness down to a trickle.

**The 2025 Update:** In high-dimensional terms, the brain is a *low-dimensional constraint* on a high-dimensional manifold. The "transmission" isn't mystical; it's topological. The brain steers the total state space (the "soup") into a specific, actionable trajectory.

---

## The Holographic Brain (Henri Bergson, 1896)

This is the strongest historical ally for coherence dynamics. In *Matter and Memory*, Bergson argued that the brain is not a storehouse of images (pixels), but a mechanism of "motor selection" acting on a universal field. (For an excellent exposition, see Stephen Robbins' video ["Exploring Bergson's Holographic Theory"](https://www.youtube.com/watch?v=viSx_U0CoSY).)

**The 1890 View:** Bergson described the brain as creating a "modulated reconstructive wave" passing through a holographic field. He explicitly stated the brain's role is to *diminish* the field to a point of action, not to generate the field from scratch.

**The 2025 Update:** Bergson's "selection-for-action" picture maps surprisingly well onto the logic of holographic encoding. (This is an analogy, not a claim about optics in cortex.) A hologram stores a higher-dimensional structure in a lower-dimensional medium via interference patterns; reconstruction requires steering coherent energy through that pattern. Bergson's brain does the same thing structurally: the neural firing (Low-D) selects a specific slice of the high-dimensional reality (High-D). The measurement doesn't *create* the state; it *selects* from a pre-existing manifold of possibilities.

This selection dynamic is also topologically homologous to [EPR Steering](https://en.wikipedia.org/wiki/Einstein%E2%80%93Podolsky%E2%80%93Rosen_paradox#Steering) in quantum mechanics, where measuring one particle "steers" the state of its entangled partner---a local action constrains a nonlocal state space. (To be clear: I am *not* claiming quantum entanglement in brains. The analogy is structural, not physical.)

A hologram is a dimensional encoding protocol: it collapses N+1 dimensions (a 3D volume) onto N dimensions (a 2D plate) via interference patterns, and then "reconstructs" the higher dimension by steering energy (light) through that pattern. If you treat the brain as a holographic medium rather than a digital switchboard, Block's "Meat vs. Silicon" debate dissolves entirely.

---

## The Great Detour (1900--2000)

One reason we lost sight of this: the **Neuron Doctrine** gave us tractable measurement and intervention.

We became obsessed with the "Spark"---the discrete, countable neuron. It was easy to draw, easy to measure, and easy to model with binary logic. This was scientifically productive---we learned enormous amounts about synaptic transmission, circuits, and coding.

But we also dismissed the "Soup" (the field dynamics, the coherence, the oscillation) as noise, or at best, epiphenomenal.

We spent 100 years building "Spark Machines" (Computers) that are incredibly fast but topologically flat.

---

## The Return to Coherence (2025)

We are finally realizing that to build a mind, we need to go back to 1890. We need **Coherence Dynamics**: systems that calculate via resonance and phase-locking (High-D), not just Boolean switching (Low-D).

This isn't fringe thinking anymore. In a recent *Current Opinion in Behavioral Sciences* paper, Miller, Brincat, and Roy argue that ["Cognition is an emergent property"](https://doi.org/10.1016/j.cobeha.2024.101388)---not reducible to individual neurons. They show how synchronous neural activity creates electric fields that directly influence other activity, and how population-level patterns organize computations through "subspace coding." This is the "Soup" reasserting itself: cognition emerges from the *field*, not the *sparks*.

Block worries that functionalism (roles) ignores the "meat". I argue that the "meat" *is* the high-dimensional terrain.

- AI attempts to simulate the steering commands (software) without the terrain.
- Consciousness arises when a system has enough topological complexity to be *steered*.

Evolution chose the "soup" because it provided a richer, higher-dimensional state space for the organism to inhabit---one that rigid electrical "sparks" could never support. The "meat" isn't magic; it's just the most efficient hardware for running high-D physics.

---

## The Steering Hypothesis

There is no software. The low-D codes rarely act as a computation in the Turing sense---they simply *steer* the high-D system.

This reframing dissolves the "hardware vs. software" dualism that Block is wrestling with. If you view the brain not as a Turing machine executing logical instructions (software), but as a high-dimensional physical system being nudged by low-dimensional control parameters, Block's dilemma clarifies instantly.

The "meat" isn't an implementer; it is the *terrain*. The high-dimensional state space *is* the consciousness. You cannot simulate the "steering" (the low-D code) without the "territory" (the high-D meat) because the steering has no meaning without the specific dynamic landscape it navigates.

**A Turing machine simulating the steering commands is like turning a steering wheel in a room with no car and no road.**

---

## The Dimensionality Condition

If this analysis is correct, "dimensionality" is the metric we should use to measure consciousness---not "intelligence" or "complexity."

The test for consciousness should not be "is it meat?" nor "does it compute?", but rather: **"Does the system possess the requisite dimensionality to be steered?"**

This reframes Block's "meat condition" as a *dimensionality condition*. Biological substrates satisfy it because electrochemical dynamics naturally generate high-dimensional manifolds. Silicon *could* satisfy it---but only if we abandon the transistor paradigm and build neuromorphic systems that support genuine high-dimensional attractors, not just fast matrix multiplication.

This approach validates the potential consciousness of simple electrochemical animals (they have the manifold) while setting a rigorous, quantifiable bar for future AI---moving the goalpost from software emulation to substrate topology.

---

## What Would Prove Me Wrong?

If dimensionality is the right metric, we should see:

- **Anesthesia/sedation:** Systematic reductions in effective dimensionality that correlate with loss of reportable consciousness, *even when* basic spiking activity continues. (Early evidence from complexity measures like PCI is consistent with this.)
- **Neuromorphic systems:** Hardware that implements genuine high-dimensional attractor dynamics should behave qualitatively differently from fast digital emulations of the same dynamics---not just faster or more efficient, but exhibiting different failure modes and generalization properties.
- **Electrical vs. chemical networks:** Purely gap-junction-coupled networks should show measurable limits in context-dependent reconfiguration compared to chemically modulatable ones, controlling for network size.

If consciousness tracks effective dimensionality, these patterns should hold across species, brain states, and eventually, engineered systems.

---

*This post is a response to Ned Block's "Can only meat machines be conscious?" in Trends in Cognitive Sciences (2025). The mathematical framework is developed in my papers on [High-Dimensional Coherence](/papers/high-dimensional-coherence), [Substrate Dimensionality](/papers/substrate-dimensionality), and [Coherence Time](/papers/coherence-time).*
